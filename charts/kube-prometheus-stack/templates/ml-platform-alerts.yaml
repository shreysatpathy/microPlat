apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "kube-prometheus-stack.fullname" . }}-ml-platform-alerts
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "kube-prometheus-stack.labels" . | nindent 4 }}
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: ml-platform.rules
    rules:
    # Ray Cluster Alerts
    - alert: RayClusterDown
      expr: up{job="ray-head"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Ray cluster head node is down"
        description: "Ray cluster head node has been down for more than 5 minutes."
    
    - alert: RayWorkerNodesDown
      expr: count(up{job="ray-workers"} == 1) < 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "No Ray worker nodes available"
        description: "All Ray worker nodes are down or unavailable."
    
    - alert: RayClusterHighCPUUsage
      expr: rate(ray_node_cpu_utilization[5m]) > 0.8
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Ray cluster high CPU usage"
        description: "Ray cluster CPU usage is above 80% for more than 10 minutes."
    
    - alert: RayClusterHighMemoryUsage
      expr: ray_node_mem_used / ray_node_mem_total > 0.9
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Ray cluster high memory usage"
        description: "Ray cluster memory usage is above 90% for more than 10 minutes."
    
    # JupyterHub Alerts
    - alert: JupyterHubDown
      expr: up{job="kubernetes-pods",kubernetes_pod_label_app="jupyterhub"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "JupyterHub is down"
        description: "JupyterHub has been down for more than 5 minutes."
    
    - alert: JupyterHubHighUserLoad
      expr: jupyterhub_total_users / jupyterhub_server_spawn_duration_seconds_count > 0.8
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "JupyterHub high user load"
        description: "JupyterHub is experiencing high user load."
    
    # MLflow Alerts
    - alert: MLflowDown
      expr: up{job="mlflow"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "MLflow tracking server is down"
        description: "MLflow tracking server has been down for more than 5 minutes."
    
    # Storage Alerts
    - alert: PersistentVolumeUsageHigh
      expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.85
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Persistent Volume usage is high"
        description: "Persistent Volume {{ $labels.persistentvolumeclaim }} usage is above 85%."
    
    - alert: PersistentVolumeUsageCritical
      expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.95
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Persistent Volume usage is critical"
        description: "Persistent Volume {{ $labels.persistentvolumeclaim }} usage is above 95%."
    
    # GPU Alerts (if applicable)
    - alert: GPUUtilizationHigh
      expr: DCGM_FI_DEV_GPU_UTIL > 95
      for: 15m
      labels:
        severity: warning
      annotations:
        summary: "GPU utilization is high"
        description: "GPU {{ $labels.gpu }} utilization is above 95% for more than 15 minutes."
    
    - alert: GPUMemoryHigh
      expr: (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL) > 0.9
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "GPU memory usage is high"
        description: "GPU {{ $labels.gpu }} memory usage is above 90% for more than 10 minutes."
    
    # Kubernetes Resource Alerts
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod is crash looping"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping."
    
    - alert: PodNotReady
      expr: kube_pod_status_ready{condition="false"} == 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod not ready"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been not ready for more than 10 minutes."
